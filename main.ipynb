{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data': 'cifar100',\n",
    "    'model': 'EfficientNetB0',\n",
    "    'lr': 0.001,\n",
    "    'epochs': 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "if args.get('data') == 'cifar10':\n",
    "    num_classes = 10\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    data_path += '/cifar10'\n",
    "    training_set = torchvision.datasets.CIFAR10(root=data_path, train=True, download=False)\n",
    "    testing_set = torchvision.datasets.CIFAR10(root=data_path, train=False, download=False, transform=test_transform)\n",
    "\n",
    "elif args.get('data') == 'cifar100':\n",
    "    num_classes = 100\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(224, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5070751592371323, 0.48654887331495095, 0.4409178433670343), (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)),\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5070751592371323, 0.48654887331495095, 0.4409178433670343), (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)),\n",
    "    ])\n",
    "    data_path += '/cifar100'\n",
    "    training_set = torchvision.datasets.CIFAR100(root=data_path, train=True, download=False)\n",
    "    testing_set = torchvision.datasets.CIFAR100(root=data_path, train=False, download=False, transform=test_transform)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "val_size = int(len(training_set)*0.1)\n",
    "train_size = len(training_set) - val_size\n",
    "train_ds, val_ds = random_split(training_set, [train_size, val_size])\n",
    "\n",
    "train_ds.dataset.transform = train_transform\n",
    "val_ds.dataset.transform = test_transform\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(testing_set, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = testing_set.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model\n",
    "model_name = args.get('model')\n",
    "\n",
    "available_models = [\n",
    "    'ResNet18', 'ResNet50', 'ResNet101', 'ResNet152', \n",
    "    'EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2', 'EfficientNetB3',\n",
    "    'EfficientNetB4', 'EfficientNetB5', 'EfficientNetB6', 'EfficientNetB7',\n",
    "    ]\n",
    "\n",
    "if model_name == 'ResNet18':\n",
    "    model = ResNet18(num_classes)\n",
    "\n",
    "elif model_name == 'ResNet34':\n",
    "    model = ResNet34(num_classes)\n",
    "\n",
    "elif model_name == 'ResNet50':\n",
    "    model = ResNet50(num_classes)\n",
    "\n",
    "elif model_name == 'ResNet101':\n",
    "    model = ResNet101(num_classes)\n",
    "\n",
    "elif model_name == 'ResNet152':\n",
    "    model = ResNet152(num_classes)\n",
    "\n",
    "elif model_name.startswith(\"EfficientNet\"):\n",
    "    model = efficientnet(model_name, num_classes=num_classes)\n",
    "else:\n",
    "    raise ValueError(f\"{model_name} is not available please use one of models below \\n{', '.join(available_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n"
     ]
    }
   ],
   "source": [
    "# setting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device = device)\n",
    "cudnn.benchmark = True\n",
    "if next(model.parameters()).is_cuda:\n",
    "    print('train on cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_max = len(train_loader) * args.get('epochs')\n",
    "print(len(train_loader))\n",
    "T_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = args.get('epochs')\n",
    "milestones = [epochs*0.3, epochs*0.6, epochs*0.8]\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=float(args.get('lr')),\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.2) #learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    n_epochs,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_name,\n",
    "    early_stop=30,\n",
    "    model_tag=\"\",\n",
    "):\n",
    "    best_val_accuracy = 0.0\n",
    "    no_improvement_count = 0\n",
    "    current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_train = 0.0\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", unit=\"batch\") as t:\n",
    "            for images, labels in t:\n",
    "                images = images.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                loss_train += loss.item()\n",
    "\n",
    "        val_accuracy, val_loss, train_accuracy = validate(\n",
    "            model, train_loader, val_loader, loss_fn\n",
    "        )\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        print(\n",
    "            f\"loss: {loss_train/(len(train_loader))}, acc: {train_accuracy}, val_loss: {val_loss/(len(val_loader))}, val_acc: {val_accuracy}, lr:{current_lr}\"\n",
    "        )\n",
    "\n",
    "        # Check if the current model has the best validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            no_improvement_count = 0\n",
    "            best_val_accuracy = val_accuracy\n",
    "            save_best_model(\n",
    "                model,\n",
    "                optimizer,\n",
    "                val_accuracy,\n",
    "                val_loss,\n",
    "                model_name,\n",
    "                model_folder=f\"saved_model_{current_datetime}\",\n",
    "                model_tag=model_tag,\n",
    "            )\n",
    "            best_model = model\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        if no_improvement_count >= early_stop:\n",
    "            print(\n",
    "                f\"Early stopping! No improvement for {early_stop} consecutive epochs.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation def\n",
    "def validate(model, train_loader, val_loader, loss_fn, testing=False):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        if name == \"train\" and testing == True:\n",
    "            train_accuracy = None\n",
    "            continue\n",
    "        else:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in loader:\n",
    "                    imgs = imgs.to(device=device)\n",
    "                    labels = labels.to(device=device)\n",
    "                    outputs = model(imgs)\n",
    "                    _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "                    total += labels.shape[0]\n",
    "                    correct += int((predicted == labels).sum())\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "            if name == \"val\":\n",
    "                val_accuracy = correct / total\n",
    "                val_loss = total_loss\n",
    "\n",
    "            if name == \"train\":\n",
    "                train_accuracy = correct / total\n",
    "\n",
    "    return val_accuracy, val_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save_best_model(\n",
    "    model, optimizer, val_accuracy, val_loss, model_name, model_folder, model_tag\n",
    "):\n",
    "    current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    model_folder += \"_\" + model_tag\n",
    "    model_filename = (\n",
    "        f\"{model_name}_{current_datetime}_acc_{val_accuracy:.4f}_loss_{val_loss:.4f}\"\n",
    "    )\n",
    "    model_path = os.path.join(model_folder, model_filename)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    for existing_model in os.listdir(model_folder):\n",
    "        existing_model_path = os.path.join(model_folder, existing_model)\n",
    "        os.remove(existing_model_path)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        model_path,\n",
    "    )\n",
    "    print(\n",
    "        f\"Saved the best model with validation accuracy: {val_accuracy:.4f}, validation loss: {val_loss:.4f} in {model_path}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strat training\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "model.train()\n",
    "best_model = training_loop(\n",
    "    n_epochs=args.epochs,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_name=model_name,\n",
    "    model_tag=f\"{args.data}_epoch_{args.epochs}_lr_{args.lr}_batchSize_{args.batch_size}_imageSize_{args.image_size}\",\n",
    "    early_stop=args.early_stop,\n",
    ")\n",
    "\n",
    "print(\"Training completed, start testing on test_dataset...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "best_model.eval()\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "test_accuracy, test_loss, _ = validate(\n",
    "    model, train_loader, test_loader, loss_fn, testing=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Average Test Loss: {test_loss / len(test_loader)}, Test Accuracy: {test_accuracy}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
